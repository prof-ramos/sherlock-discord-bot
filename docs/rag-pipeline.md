# ğŸ“š RAG Pipeline - Sherlock Discord Bot

## VisÃ£o Geral

O **Retrieval-Augmented Generation (RAG)** Ã© o sistema que permite ao SherlockBot responder
perguntas baseando-se em documentos reais da sua base de conhecimento jurÃ­dico.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Documento  â”‚ â”€â”€â–¶  â”‚   Chunking   â”‚ â”€â”€â–¶  â”‚  Embeddings â”‚
â”‚  (PDF/TXT)  â”‚      â”‚   (Tokens)   â”‚      â”‚   (OpenAI)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Resposta  â”‚ â—€â”€â”€  â”‚   LLM + RAG  â”‚ â—€â”€â”€  â”‚   pgvector  â”‚
â”‚   (Bot)     â”‚      â”‚   Context    â”‚      â”‚   (Neon DB) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Arquitetura

### Componentes Principais

| Componente            | Arquivo                  | DescriÃ§Ã£o                        |
| --------------------- | ------------------------ | -------------------------------- |
| **Ingestion Script**  | `scripts/ingest_docs.py` | Processa e indexa documentos     |
| **RAG Service**       | `src/rag_service.py`     | Busca hÃ­brida (Vector + Keyword) |
| **Embedding Service** | `src/rag_service.py`     | Gera embeddings via OpenAI       |
| **Completion**        | `src/completion.py`      | Injeta contexto RAG nos prompts  |

### Tecnologias

- **Banco de Dados**: Neon (PostgreSQL Serverless)
- **Vector Search**: pgvector (HNSW index)
- **Full-Text Search**: PostgreSQL tsvector (portuguÃªs)
- **Embeddings**: OpenAI `text-embedding-3-small` (1536 dimensÃµes)
- **Ranking**: Reciprocal Rank Fusion (RRF)

---

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

```bash
# ObrigatÃ³rias
DATABASE_URL=postgresql://user:pass@host/db
OPENAI_API_KEY=sk-...

# Opcionais
EMBEDDING_MODEL=text-embedding-3-small   # Modelo de embeddings
TEXT_SEARCH_LANG=portuguese               # Idioma para full-text search
```

### InicializaÃ§Ã£o do Banco

Execute o schema de inicializaÃ§Ã£o:

```bash
psql $DATABASE_URL -f scripts/init_rag_pgvector.sql
```

Este script cria:

- ExtensÃ£o `pgvector`
- Tabela `documents` com coluna `embedding vector(1536)`
- Ãndice HNSW para busca vetorial rÃ¡pida
- Ãndice GIN para full-text search

---

## ğŸ“„ Alimentando a Base de Conhecimento

### Formatos Suportados

| Formato | ExtensÃ£o      | Requisitos                 |
| ------- | ------------- | -------------------------- |
| PDF     | `.pdf`        | `pypdf` instalado          |
| Word    | `.docx`       | `python-docx` instalado    |
| HTML    | `.html`       | `beautifulsoup4` instalado |
| Texto   | `.txt`, `.md` | Nenhum                     |

### Comando de IngestÃ£o

```bash
# Ingerir um documento
uv run python scripts/ingest_docs.py caminho/para/documento.pdf

# Com parÃ¢metros customizados
uv run python scripts/ingest_docs.py documento.pdf --chunk-size 800 --overlap 150
```

### ParÃ¢metros

| ParÃ¢metro      | Default | DescriÃ§Ã£o                             |
| -------------- | ------- | ------------------------------------- |
| `--chunk-size` | 1000    | Tamanho mÃ¡ximo do chunk (em tokens)   |
| `--overlap`    | 200     | SobreposiÃ§Ã£o entre chunks (em tokens) |

### Exemplo PrÃ¡tico

```bash
# 1. Ingerir a ConstituiÃ§Ã£o Federal
uv run python scripts/ingest_docs.py ~/docs/constituicao_federal.pdf

# 2. Ingerir CÃ³digo Civil
uv run python scripts/ingest_docs.py ~/docs/codigo_civil.pdf --chunk-size 500

# 3. Verificar status
uv run python scripts/verify_ingestion.py
```

---

## ğŸ” Como Funciona a Busca

### 1. Hybrid Search (Busca HÃ­brida)

O sistema combina duas estratÃ©gias de busca:

#### Vector Search (SemÃ¢ntica)

```sql
SELECT content FROM documents
ORDER BY embedding <=> $query_embedding::vector
LIMIT 10;
```

- Encontra documentos semanticamente similares
- Usa distÃ¢ncia de cosseno (`<=>`)
- Ãndice HNSW para performance

#### Keyword Search (Full-Text)

```sql
SELECT content FROM documents
WHERE content_search @@ websearch_to_tsquery('portuguese', $query)
ORDER BY ts_rank(content_search, ...) DESC
LIMIT 10;
```

- Encontra matches exatos de palavras
- Suporta linguagem natural
- Otimizado para portuguÃªs

### 2. Reciprocal Rank Fusion (RRF)

Os resultados das duas buscas sÃ£o combinados usando RRF:

```python
score = 1 / (k + rank)  # k = 60 (padrÃ£o da indÃºstria)
```

Isso garante que documentos que aparecem bem em **ambas** as buscas sejam priorizados.

### 3. InjeÃ§Ã£o de Contexto

O contexto recuperado Ã© formatado em XML e injetado no prompt:

```xml
<relevant_context>
  <doc index='1'>ConteÃºdo do documento mais relevante...</doc>
  <doc index='2'>Segundo documento mais relevante...</doc>
</relevant_context>

Instructions:
1. Use the provided context to answer the user's question.
2. If the context contains the answer, cite the document index.
3. If insufficient, use general knowledge but mention missing details.
```

---

## ğŸ“Š Monitoramento

### Verificar EstatÃ­sticas

```bash
uv run python scripts/verify_ingestion.py
```

### Teste End-to-End

```bash
uv run python scripts/test_rag_e2e.py
```

SaÃ­da esperada:

```
ğŸ§ª RAG End-to-End Test
ğŸ“Š Status: active
ğŸ“„ Documents ingested: âœ…
ğŸ” Query returned 3 results
âœ¨ RRF correctly ranked document!
âœ… RAG End-to-End Test Completed!
```

---

## ğŸš€ Boas PrÃ¡ticas

### Chunking

1. **Chunk Size**: 500-1000 tokens Ã© ideal para contexto legal
2. **Overlap**: 10-20% do chunk_size previne perda de contexto
3. **Documentos longos**: Use `RecursiveTokenSplitter` (automÃ¡tico)

### Performance

1. **HNSW Index**: JÃ¡ configurado com `m=16, ef_construction=64`
2. **Connection Pool**: Limitado a 5 conexÃµes para Neon Free Tier
3. **Batch Ingestion**: Documentos sÃ£o processados em lote

### Qualidade

1. **HÃ­brido > Vector-only**: A busca hÃ­brida melhora recall
2. **RRF k=60**: Valor padrÃ£o balanceia precisÃ£o e diversidade
3. **Top-5 Results**: Mais contexto nem sempre Ã© melhor

---

## ğŸ”§ Troubleshooting

### "OPENAI_API_KEY not found"

```bash
export OPENAI_API_KEY=sk-your-key-here
```

### "DATABASE_URL not set"

```bash
export DATABASE_URL=postgresql://user:pass@host/dbname
```

### "No documents found"

1. Verifique se os documentos foram ingeridos
2. Execute `scripts/verify_ingestion.py`
3. Confirme que o schema foi inicializado

### Embeddings lentos

- O modelo `text-embedding-3-small` Ã© o mais rÃ¡pido
- Batch processing jÃ¡ estÃ¡ implementado
- Considere aumentar rate limits da OpenAI

---

## ğŸ“ Estrutura de Arquivos

```
sherlock-discord-bot/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init_rag_pgvector.sql    # Schema do banco
â”‚   â”œâ”€â”€ ingest_docs.py           # Script de ingestÃ£o
â”‚   â”œâ”€â”€ verify_ingestion.py      # VerificaÃ§Ã£o de status
â”‚   â””â”€â”€ test_rag_e2e.py          # Teste end-to-end
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rag_service.py           # ServiÃ§o RAG principal
â”‚   â”œâ”€â”€ completion.py            # InjeÃ§Ã£o de contexto
â”‚   â””â”€â”€ database.py              # ConexÃ£o com Neon
â””â”€â”€ docs/
    â””â”€â”€ rag-pipeline.md          # Esta documentaÃ§Ã£o
```
